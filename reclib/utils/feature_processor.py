import torch
from torch import nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union, Any, Tuple
import json
import pickle
from collections import defaultdict, Counter
import warnings
from tqdm import tqdm
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
warnings.filterwarnings('ignore')

# todo ç¼–ç ç­‰è¿‡ç¨‹å‡åœ¨pd.Dataframeä¸Šè¿›è¡Œ
class FeatureMap:
    """ç‰¹å¾æ˜ å°„ç±»ï¼Œç®¡ç†æ‰€æœ‰ç‰¹å¾çš„ä¿¡æ¯"""
    
    def __init__(self, data_dir: str):
        self.data_dir = data_dir # æ•°æ®å­˜å‚¨è·¯å¾„ï¼ˆå¦‚"./data/movie"ï¼‰
        self.features = {} # å­˜å‚¨æ‰€æœ‰ç‰¹å¾çš„è¯¦ç»†ä¿¡æ¯ï¼ˆé”®ï¼šç‰¹å¾åï¼Œå€¼ï¼šç‰¹å¾å…ƒä¿¡æ¯å­—å…¸ï¼‰ã€‚
        # self.feature_type = {}
        # self.feature_dim = {}
        # self.embedding_dim = {}
        self.sequence_features = {} # å­˜å‚¨åºåˆ—ç‰¹å¾çš„æœ€å¤§é•¿åº¦å’Œæ± åŒ–æ–¹å¼ï¼ˆé”®ï¼šç‰¹å¾åï¼Œå€¼ï¼š{"max_length": ..., "pooling": ...}ï¼‰ã€‚
        self.numerical_features = [] # æ•°å€¼ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚[â€œageâ€, â€œincomeâ€]ï¼‰
        self.categorical_features = [] # ç±»åˆ«ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚[â€œgenderâ€, â€œoccupationâ€]ï¼‰
        self.condition_features = [] # æ¡ä»¶ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚[â€œcontext_locationâ€]ï¼Œç”¨äºæ¨¡å‹æ¡ä»¶è¾“å…¥ï¼‰
        
    def add_feature(self, feature_name: str, feature_type: str, 
                   feature_dim: int | None = None, embedding_dim: int = 10,
                   is_sequence: bool = False, max_length: int = 50,
                   pooling: str = "mean", is_condition: bool = False):
        """æ·»åŠ ç‰¹å¾ä¿¡æ¯
        å°†ç‰¹å¾å…ƒä¿¡æ¯å­˜å…¥self.featureså­—å…¸ï¼ˆç»Ÿä¸€ç®¡ç†æ‰€æœ‰ç‰¹å¾ç»†èŠ‚ï¼‰ã€‚
        æ ¹æ®feature_typeå°†ç‰¹å¾åæ·»åŠ åˆ°numerical_featuresæˆ–categorical_featuresåˆ—è¡¨ã€‚
        è‹¥is_sequence=Trueï¼Œå°†ç‰¹å¾ååŠmax_length/poolingå­˜å…¥sequence_featureså­—å…¸ã€‚
        è‹¥is_condition=Trueï¼Œå°†ç‰¹å¾åæ·»åŠ åˆ°condition_featuresåˆ—è¡¨ã€‚
        """
        self.features[feature_name] = {
            "type": feature_type, # ç‰¹å¾ç±»å‹ï¼š"numerical"ï¼ˆæ•°å€¼å‹ï¼‰æˆ–"categorical"ï¼ˆç±»åˆ«å‹ï¼‰
            "dim": feature_dim, # ç‰¹å¾ç»´åº¦ï¼ˆç±»åˆ«ç‰¹å¾ï¼šç±»åˆ«æ€»æ•°ï¼›æ•°å€¼ç‰¹å¾ï¼šå¯å¿½ç•¥ï¼Œé»˜è®¤ä¸ºNoneï¼‰
            "embedding_dim": embedding_dim, # ç±»åˆ«ç‰¹å¾åµŒå…¥ç»´åº¦
            "is_sequence": is_sequence, # æ˜¯å¦ä¸ºåºåˆ—ç‰¹å¾ï¼ˆå¦‚ç”¨æˆ·ç‚¹å‡»å†å²åºåˆ—ï¼‰
            "max_length": max_length, # åºåˆ—ç‰¹å¾çš„æœ€å¤§é•¿åº¦ï¼ˆè¶…è¿‡åˆ™æˆªæ–­ï¼Œä¸è¶³åˆ™å¡«å……ï¼‰
            "pooling": pooling, # åºåˆ—ç‰¹å¾çš„æ± åŒ–æ–¹å¼ï¼ˆå¦‚"mean"å¹³å‡æ± åŒ–ã€"sum"æ±‚å’Œæ± åŒ–ã€"max"æœ€å¤§æ± åŒ–ï¼‰
            "is_condition": is_condition # æ˜¯å¦ä¸ºæ¡ä»¶ç‰¹å¾ï¼ˆç”¨äºæ¨¡å‹çš„æ¡ä»¶è¾“å…¥ï¼Œå¦‚ä¸Šä¸‹æ–‡ç‰¹å¾ï¼‰
        }
        
        if feature_type == "categorical":
            self.categorical_features.append(feature_name)
        elif feature_type == "numerical":
            self.numerical_features.append(feature_name)
        
        if is_sequence:
            self.sequence_features[feature_name] = {
                "max_length": max_length,
                "pooling": pooling
            }
            
        if is_condition:
            self.condition_features.append(feature_name)
    
    def get_feature_dim(self, feature_name: str) -> int:
        """è·å–ç‰¹å¾ç»´åº¦"""
        return self.features[feature_name]["dim"]
    
    def get_embedding_dim(self, feature_name: str) -> int:
        """è·å–åµŒå…¥ç»´åº¦"""
        return self.features[feature_name]["embedding_dim"]
    
    def is_sequence_feature(self, feature_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºåºåˆ—ç‰¹å¾"""
        return self.features[feature_name]["is_sequence"]
    
    def sum_emb_out_dim(self) -> int:
        """è®¡ç®—æ‰€æœ‰ç‰¹å¾åµŒå…¥åçš„æ€»ç»´åº¦"""
        total_dim = 0
        for feature_name in self.features:
            if self.is_sequence_feature(feature_name):
                # åºåˆ—ç‰¹å¾ç»è¿‡æ± åŒ–åçš„ç»´åº¦
                total_dim += self.get_embedding_dim(feature_name)
            else:
                # æ™®é€šç‰¹å¾çš„åµŒå…¥ç»´åº¦
                total_dim += self.get_embedding_dim(feature_name)
        return total_dim
    
    def save(self, filepath: str):
        """ä¿å­˜ç‰¹å¾æ˜ å°„"""
        with open(filepath, 'w') as f:
            json.dump(self.features, f, indent=2)
    
    def load(self, filepath: str, params: dict | None = None):
        """åŠ è½½ç‰¹å¾æ˜ å°„"""
        with open(filepath, 'r') as f:
            self.features = json.load(f)
        
        # é‡å»ºç´¢å¼•
        self.categorical_features = []
        self.numerical_features = []
        self.sequence_features = {}
        self.condition_features = []
        
        for feature_name, feature_info in self.features.items():
            if feature_info["type"] == "categorical":
                self.categorical_features.append(feature_name)
            elif feature_info["type"] == "numerical":
                self.numerical_features.append(feature_name)
            
            if feature_info["is_sequence"]:
                self.sequence_features[feature_name] = {
                    "max_length": feature_info["max_length"],
                    "pooling": feature_info["pooling"]
                }
            
            if feature_info["is_condition"]:
                self.condition_features.append(feature_name)


class SequenceVocabulary:
    """ç”Ÿäº§çº§åºåˆ—è¯æ±‡è¡¨"""
    
    def __init__(self,
                 min_freq: int = 2,
                 max_size: int = 50000,
                 unk_token: str = '<UNK>',
                 pad_token: str = '<PAD>',
                 bos_token: str = '<BOS>',
                 eos_token: str = '<EOS>',
                 mask_token: str = '<MASK>'):
        
        # ç‰¹æ®Štokenç®¡ç†
        self.special_tokens = {
            pad_token: 0, unk_token: 1,
            bos_token: 2, eos_token: 3, mask_token: 4
        }
        
        # è¯æ±‡è¡¨ç»Ÿè®¡
        self.token_counts = Counter() # Counter å¯¹è±¡ï¼Œç»Ÿè®¡æ™®é€š token çš„å‡ºç°é¢‘ç‡
        self.vocab = {} # æ™®é€š tokenâ†’ç´¢å¼•çš„æ˜ å°„è¡¨ï¼ˆä¸åŒ…å«ç‰¹æ®Šæ ‡è®°ï¼‰
        self.reverse_vocab = {} # æ™®é€š tokenâ†’ç´¢å¼•çš„æ˜ å°„è¡¨ï¼ˆä¸åŒ…å«ç‰¹æ®Šæ ‡è®°ï¼‰
        
        # é…ç½®
        self.min_freq = min_freq # 	æœ€å°è¯é¢‘é˜ˆå€¼ï¼šä»…é¢‘ç‡ â‰¥ è¯¥å€¼çš„ token æ‰ä¼šè¢«åŠ å…¥è¯æ±‡è¡¨
        self.max_size = max_size # 	è¯æ±‡è¡¨æ€»å¤§å°ä¸Šé™ï¼ˆåŒ…å«ç‰¹æ®Šæ ‡è®°ï¼‰ï¼Œé»˜è®¤ 50000
        
        # ç‰¹æ®Štokenåç§°
        self.unk_token = unk_token
        self.pad_token = pad_token
        self.bos_token = bos_token
        self.eos_token = eos_token
        self.mask_token = mask_token
    
    def build_vocab(self, sequences: List[List[str]]):
        """
        ä»åºåˆ—åˆ—è¡¨æ„å»ºè¯æ±‡è¡¨
        ç»Ÿè®¡åŸå§‹æ–‡æœ¬åºåˆ—ä¸­æ‰€æœ‰æ™®é€š token çš„é¢‘ç‡ï¼›
        æŒ‰é¢‘ç‡ä»é«˜åˆ°ä½æ’åºï¼Œè¿‡æ»¤ä½é¢‘è¯ï¼ˆé¢‘ç‡ < min_freqï¼‰ï¼Œå¹¶é™åˆ¶æ€»è¯æ±‡è¡¨å¤§å° â‰¤ max_sizeï¼›
        æ„å»ºæ™®é€š token ä¸ç´¢å¼•çš„æ˜ å°„ï¼ˆvocab å’Œ reverse_vocabï¼‰
        """
        # ç»Ÿè®¡è¯é¢‘
        for sequence in sequences:
            for token in sequence:
                self.token_counts[token] += 1
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œè¿‡æ»¤ä½é¢‘è¯
        sorted_tokens = sorted(
            self.token_counts.items(),
            key=lambda x: x[1],
            reverse=True  # é™åºï¼ˆé«˜é¢‘åœ¨å‰ï¼‰
        )
        
        # æ„å»ºè¯æ±‡è¡¨ï¼Œè€ƒè™‘å¤§å°é™åˆ¶
        idx = len(self.special_tokens)
        for token, count in sorted_tokens:
            if count >= self.min_freq and idx < self.max_size:
                self.vocab[token] = idx
                self.reverse_vocab[idx] = token
                idx += 1
            else:
                break  # è¾¾åˆ°å¤§å°é™åˆ¶
    
    def encode(self, tokens: List[str]) -> List[int]:
        """ç¼–ç tokenåºåˆ—ï¼Œå°†è¾“å…¥çš„ token åˆ—è¡¨è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨ï¼Œæœªæ”¶å½•çš„ token ç”¨ <UNK> çš„ç´¢å¼•ï¼ˆ1ï¼‰æ›¿ä»£ã€‚"""
        return [
            self.vocab.get(token, self.special_tokens[self.unk_token])
            for token in tokens
        ]
    
    def decode(self, indices: List[int]) -> List[str]:
        """è§£ç ç´¢å¼•åºåˆ—"""
        return [
            self.reverse_vocab.get(idx, self.unk_token)
            for idx in indices
        ]
    
    def __len__(self):
        return len(self.vocab) + len(self.special_tokens)


class SequencePreprocessor:
    """åºåˆ—é¢„å¤„ç†ç®¡é“, ç”¨äºå°†è¾“å…¥åºåˆ—ï¼ˆå¦‚å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–æ•°ç»„ï¼‰é€šè¿‡ä¸€ç³»åˆ—æ ‡å‡†åŒ–æ“ä½œï¼ˆå¦‚å°å†™è½¬æ¢ã€æ ‡ç‚¹ç§»é™¤ã€é•¿åº¦è¿‡æ»¤ç­‰ï¼‰è½¬æ¢ä¸ºå¹²å‡€çš„ token åˆ—è¡¨ï¼Œé€‚ç”¨äºæ–‡æœ¬å¤„ç†ã€ç‰¹å¾æå–ç­‰åœºæ™¯ã€‚"""
    
    def __init__(self,
                 lowercase: bool = True,
                 remove_punct: bool = True,
                 min_token_length: int = 1,
                 split_by: str = ','):
        
        self.lowercase = lowercase # æ˜¯å¦å°† token è½¬ä¸ºå°å†™ï¼ˆé»˜è®¤ Trueï¼‰
        self.remove_punct = remove_punct # æ˜¯å¦ç§»é™¤éå­—æ¯/æ•°å­—å­—ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ã€ç©ºæ ¼ï¼Œé»˜è®¤ Trueï¼‰
        self.min_token_length = min_token_length # token æœ€å°ä¿ç•™é•¿åº¦ï¼ˆé»˜è®¤ 1ï¼ŒçŸ­äºè¯¥é•¿åº¦çš„ token ä¼šè¢«è¿‡æ»¤ï¼‰
        self.split_by = split_by # å­—ç¬¦ä¸²åºåˆ—çš„åˆ†å‰²ç¬¦ï¼ˆé»˜è®¤ ','ï¼Œç”¨äºå°†å­—ç¬¦ä¸²åˆ‡åˆ†ä¸ºåŸå§‹ tokenï¼‰
    
    def normalize_token(self, token: str) -> str:
        """æ ‡å‡†åŒ–å•ä¸ªtoken"""
        if self.lowercase:
            token = token.lower()
        if self.remove_punct:
            token = ''.join(c for c in token if c.isalnum())
        if len(token) < self.min_token_length:
            return ''
        return token.strip()
    
    def preprocess_sequence(self, sequence) -> List[str]:
        """é¢„å¤„ç†å•ä¸ªåºåˆ—"""
        tokens = []
        
        if isinstance(sequence, str):
            # å­—ç¬¦ä¸²åºåˆ—
            raw_tokens = sequence.split(self.split_by)
        elif isinstance(sequence, (list, np.ndarray)):
            # åˆ—è¡¨/æ•°ç»„åºåˆ—
            raw_tokens = [str(x) for x in sequence]
        else:
            return []
        
        # æ ‡å‡†åŒ–æ¯ä¸ªtoken
        for token in raw_tokens:
            normalized = self.normalize_token(token)
            if normalized:
                tokens.append(normalized)
        
        return tokens


class SequenceFeatureProcessor:
    """å®Œæ•´çš„åºåˆ—ç‰¹å¾å¤„ç†å™¨"""
    
    def __init__(self,
                 feature_name: str,
                 max_length: int = 100,
                 pooling: str = 'mean',
                 vocab_config: dict | None = None,
                 preprocessor_config: dict | None = None,
                 quiet: bool = True):
        
        self.feature_name = feature_name
        self.max_length = max_length
        self.pooling = pooling
        
        # åˆå§‹åŒ–ç»„ä»¶
        vocab_config = vocab_config or {}
        self.vocab = SequenceVocabulary(**vocab_config)
        self.preprocessor = SequencePreprocessor(**(preprocessor_config or {}))
        
        # çŠ¶æ€
        self.is_fitted = False
        self.quiet = quiet  # True æ—¶å…³é—­é€æ ·æœ¬ tqdm
    
    def fit(self, sequences: pd.Series):
        """æ‹Ÿåˆåºåˆ—ç‰¹å¾å¤„ç†å™¨"""
        print(f"ğŸ”§ Fitting sequence feature: {self.feature_name}")
        
        # é¢„å¤„ç†æ‰€æœ‰åºåˆ—
        processed_sequences = []
        iterator = sequences if self.quiet else tqdm(sequences, desc=f"Preprocessing {self.feature_name}")
        for seq in iterator:
            tokens = self.preprocessor.preprocess_sequence(seq)
            processed_sequences.append(tokens)
        
        # æ„å»ºè¯æ±‡è¡¨
        self.vocab.build_vocab(processed_sequences)
        self.is_fitted = True
        
        print(f"âœ… Vocabulary built: {len(self.vocab.vocab)} tokens")
        return self
    
    def transform(self, sequences: pd.Series) -> torch.Tensor:
        """è½¬æ¢åºåˆ—ç‰¹å¾"""
        if not self.is_fitted:
            raise ValueError("Sequence processor must be fitted before transform")
        
        batch_sequences = []
        
        iterator = sequences if self.quiet else tqdm(sequences, desc=f"Transforming {self.feature_name}")
        for seq in iterator:
            # é¢„å¤„ç†
            tokens = self.preprocessor.preprocess_sequence(seq)
            
            # ç¼–ç 
            encoded = self.vocab.encode(tokens)
            
            # æˆªæ–­
            if len(encoded) > self.max_length:
                encoded = encoded[:self.max_length]
            
            batch_sequences.append(encoded)
        
        # NOTE:
        # ä¸ºä¿è¯å¯å †å ï¼Œè¿™é‡Œå¯¹äºå•æ ·æœ¬åœºæ™¯å›ºå®šè¾“å‡ºé•¿åº¦ = self.max_length çš„ 1D å‘é‡ï¼›
        # å¯¹äºæ‰¹é‡åœºæ™¯å†è¿›è¡Œ batch çº§ paddingï¼Œå¹¶åŒæ ·è£å‰ª/å¡«å……åˆ° self.max_lengthï¼Œ

        if len(batch_sequences) == 1:
            seq = batch_sequences[0]
            pad_id = self.vocab.special_tokens[self.vocab.pad_token]
            if len(seq) < self.max_length:
                seq = seq + [pad_id] * (self.max_length - len(seq))
            # å¦‚æœå·²ç»ç­‰äºæˆ–è¶…è¿‡ max_lengthï¼Œä¸Šé¢æˆªæ–­å·²å¤„ç†
            return torch.LongTensor(seq)  # shape: (max_length,)

        # å¤šæ ·æœ¬ï¼šå…ˆåŠ¨æ€æ‰¾ batch å†…æœ€å¤§é•¿åº¦ï¼Œå†è£å‰ªåˆ° self.max_lengthï¼Œå†ç»Ÿä¸€ pad
        batch_max_len = min(self.max_length, max(len(s) for s in batch_sequences) if batch_sequences else self.max_length)
        pad_id = self.vocab.special_tokens[self.vocab.pad_token]
        padded = []
        for seq in batch_sequences:
            if len(seq) < batch_max_len:
                seq = seq + [pad_id] * (batch_max_len - len(seq))
            else:
                seq = seq[:batch_max_len]
            padded.append(seq)
        tensor = torch.LongTensor(padded)  # (batch, L)
        if tensor.shape[0] == 1:
            return tensor[0]
        return tensor
    
    def _dynamic_padding(self, sequences: List[List[int]]) -> torch.Tensor:
        """åŠ¨æ€paddingåˆ°batchå†…æœ€å¤§é•¿åº¦"""
        # å°†è¾“å…¥çš„åºåˆ—åˆ—è¡¨ï¼ˆList[List[int]]ï¼‰å¤„ç†ä¸ºé•¿åº¦ä¸€è‡´çš„åºåˆ—ï¼ˆå¡«å……æˆ–æˆªæ–­ï¼‰ï¼Œç¡®ä¿æ‰¹æ¬¡å†…æ‰€æœ‰åºåˆ—é•¿åº¦ç­‰äºè¯¥æ‰¹æ¬¡çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè‡³å°‘ä¸º1ï¼‰ï¼Œæœ€ç»ˆè½¬æ¢ä¸º torch.LongTensor è¿”å›ã€‚
        # ä¿è¯è‡³å°‘é•¿åº¦ä¸º1ï¼Œé¿å…å‡ºç° shape (1,0) é€ æˆåç»­ stack æŠ¥é”™
        batch_max_len = max(1, max(len(seq) for seq in sequences) if sequences else 1)
        padded_sequences = []
        
        for seq in sequences:
            if len(seq) < batch_max_len:
                pad_id = self.vocab.special_tokens[self.vocab.pad_token]
                padded = seq + [pad_id] * (batch_max_len - len(seq))
            else:
                padded = seq[:batch_max_len]
            padded_sequences.append(padded)
        
        return torch.LongTensor(padded_sequences)
    
    def save(self, filepath: str):
        """ä¿å­˜åºåˆ—ç‰¹å¾å¤„ç†å™¨"""
        # å°†åºåˆ—ç‰¹å¾å¤„ç†å™¨çš„æ‰€æœ‰çŠ¶æ€ï¼ˆè¯æ±‡è¡¨ã€é¢„å¤„ç†å™¨é…ç½®ã€æ‹ŸåˆçŠ¶æ€ç­‰ï¼‰ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä»¥ä¾¿åç»­åŠ è½½å¤ç”¨
        processor_data = {
            'feature_name': self.feature_name,
            'max_length': self.max_length,
            'pooling': self.pooling,
            'vocab_state': {
                'token_counts': dict(self.vocab.token_counts),
                'vocab': dict(self.vocab.vocab),
                'reverse_vocab': dict(self.vocab.reverse_vocab),
                'min_freq': self.vocab.min_freq,
                'max_size': self.vocab.max_size,
                'special_tokens': self.vocab.special_tokens,
                'unk_token': self.vocab.unk_token,
                'pad_token': self.vocab.pad_token,
                'bos_token': self.vocab.bos_token,
                'eos_token': self.vocab.eos_token,
                'mask_token': self.vocab.mask_token
            },
            'preprocessor_config': self.preprocessor.__dict__,
            'is_fitted': self.is_fitted
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(processor_data, f)
    
    def load(self, filepath: str):
        """åŠ è½½åºåˆ—ç‰¹å¾å¤„ç†å™¨"""
        with open(filepath, 'rb') as f:
            processor_data = pickle.load(f)
        
        self.feature_name = processor_data['feature_name']
        self.max_length = processor_data['max_length']
        self.pooling = processor_data['pooling']
        self.is_fitted = processor_data['is_fitted']
        
        # é‡å»ºè¯æ±‡è¡¨çŠ¶æ€
        vocab_state = processor_data['vocab_state']
        self.vocab.token_counts = Counter(vocab_state['token_counts'])
        self.vocab.vocab = vocab_state['vocab']
        self.vocab.reverse_vocab = vocab_state['reverse_vocab']
        self.vocab.min_freq = vocab_state['min_freq']
        self.vocab.max_size = vocab_state['max_size']
        self.vocab.special_tokens = vocab_state['special_tokens']
        self.vocab.unk_token = vocab_state['unk_token']
        self.vocab.pad_token = vocab_state['pad_token']
        self.vocab.bos_token = vocab_state['bos_token']
        self.vocab.eos_token = vocab_state['eos_token']
        self.vocab.mask_token = vocab_state['mask_token']
        
        # é‡å»ºé¢„å¤„ç†å™¨
        self.preprocessor.__dict__.update(processor_data['preprocessor_config'])


class FeatureProcessor:
    """é‡æ„åçš„ç‰¹å¾å¤„ç†å™¨"""
    # FeatureProcessor æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç‰¹å¾å¤„ç†å™¨ï¼Œç”¨äºè‡ªåŠ¨åŒ–å¤„ç†ç»“æ„åŒ–æ•°æ®ä¸­çš„ åˆ†ç±»ç‰¹å¾ã€æ•°å€¼ç‰¹å¾ å’Œ åºåˆ—ç‰¹å¾ã€‚
    # æ ¸å¿ƒåŠŸèƒ½æ˜¯é€šè¿‡ fit æ–¹æ³•åˆ©ç”¨è®­ç»ƒæ•°æ®æ‹Ÿåˆç‰¹å¾å¤„ç†å™¨ï¼ˆç¼–ç å™¨ã€ç¼©æ”¾å™¨ç­‰ï¼‰ï¼Œå¹¶ç”Ÿæˆå¯ç›´æ¥ç”¨äºæ¨¡å‹çš„ vocabsï¼ˆç±»åˆ«/åºåˆ— token æ˜ å°„è¡¨ï¼‰å’Œ scalersï¼ˆæ•°å€¼ç‰¹å¾ç»Ÿè®¡é‡ï¼‰ï¼Œ
    # ä¸ºåç»­ç‰¹å¾è½¬æ¢ï¼ˆå¦‚ç¼–ç ã€å½’ä¸€åŒ–ã€åºåˆ—å‘é‡åŒ–ï¼‰æä¾›åŸºç¡€ã€‚
    
    def __init__(self, feature_map: FeatureMap):
        self.feature_map = feature_map
        
        # åˆ†ç±»å‹çš„å¤„ç†å™¨
        self.categorical_encoders = {}
        self.numerical_scalers = {}
        
        # åºåˆ—ç‰¹å¾å¤„ç†å™¨
        self.sequence_processors = {}
        
        # å…¼å®¹æ—§è®­ç»ƒè„šæœ¬å¼•ç”¨ï¼švocabs / scalers
        # vocabs: {feature_name: {token: index}}
        # scalers: {feature_name: {mean: float, std: float}}
        self.vocabs = {}
        self.scalers = {}
        
        # çŠ¶æ€
        self.is_fitted = False
    
    def fit(self, data: pd.DataFrame):
        """ä½¿ç”¨è®­ç»ƒæ•°æ®æ‹Ÿåˆæ‰€æœ‰ç‰¹å¾å¤„ç†å™¨"""
        print("ğŸ”§ Fitting all feature processors (unified progress)...")
        fit_tasks: List[tuple[str, str]] = []
        for f in self.feature_map.categorical_features:
            if not self.feature_map.is_sequence_feature(f):
                fit_tasks.append(("cat", f))
        for f in self.feature_map.numerical_features:
            fit_tasks.append(("num", f))
        for f in self.feature_map.sequence_features.keys():
            fit_tasks.append(("seq", f))
        pbar = tqdm(total=len(fit_tasks), desc="Fitting features", unit="feat")

        def _prepare_cat(col: pd.Series) -> pd.Series:
            if getattr(col.dtype, 'name', '').startswith('category'):
                if '<UNK>' not in col.cat.categories:
                    col = col.cat.add_categories(['<UNK>'])
                col = col.fillna('<UNK>')
            else:
                col = col.fillna('<UNK>')
            return col

        # Categorical
        for ftype, feat in [t for t in fit_tasks if t[0]=="cat"]:
            enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, encoded_missing_value=-1)
            series = _prepare_cat(data[feat].copy())
            enc.fit(pd.DataFrame({feat: series}))
            self.categorical_encoders[feat] = enc
            raw_cats = enc.categories_[0]
            cats_np = np.asarray(raw_cats, dtype=object)
            cats_flat = cats_np.tolist()
            if not isinstance(cats_flat, list):
                cats_flat = [cats_flat]
            # ç¡®ä¿äºŒç»´åµŒå¥—åœºæ™¯ï¼ˆæå°‘è§ï¼‰å±•å¼€ä¸ºä¸€ç»´
            if cats_flat and isinstance(cats_flat[0], list):
                cats_flat = cats_flat[0]
            # ç¡®ä¿å¯æ˜ å°„åˆ° python åŸç”Ÿç±»å‹ï¼Œé¿å… numpy scalar å¸¦æ¥çš„å¯å“ˆå¸Œæ€§é—®é¢˜
            normalized_cats: List[Any] = []
            for cat in cats_flat:
                if hasattr(cat, 'item'):
                    try:
                        normalized_cats.append(cat.item())
                        continue
                    except Exception:
                        pass
                normalized_cats.append(cat)
            cats_list = normalized_cats
            self.feature_map.features[feat]['dim'] = len(cats_list)
            mapping = {cat: idx for idx, cat in enumerate(cats_list)}
            # ä¸ºæœªçŸ¥å€¼ä¿ç•™å®‰å…¨æ˜ å°„ï¼šå¦‚ä¸å­˜åœ¨ <UNK>ï¼Œåˆ™å°†å…¶æ˜ å°„åˆ° 0 å·ï¼ˆä¸ä¼šæ‰©å¢ç»´åº¦ï¼‰
            if '<UNK>' not in mapping and len(cats_list) > 0:
                mapping['<UNK>'] = 0
            self.vocabs[feat] = mapping
            pbar.set_postfix_str(f"cat:{feat}")
            pbar.update(1)

        # Numerical
        for ftype, feat in [t for t in fit_tasks if t[0]=="num"]:
            scaler = StandardScaler()
            scaler.fit(data[[feat]].fillna(0))
            self.numerical_scalers[feat] = scaler
            self.scalers[feat] = {
                'mean': float(getattr(scaler,'mean_', [0.0])[0]),
                'std': float(getattr(scaler,'scale_', [1.0])[0])
            }
            pbar.set_postfix_str(f"num:{feat}")
            pbar.update(1)

        # Sequence
        for ftype, feat in [t for t in fit_tasks if t[0]=="seq"]:
            cfg = self.feature_map.sequence_features[feat]
            sp = SequenceFeatureProcessor(feature_name=feat, max_length=cfg['max_length'], pooling=cfg['pooling'])
            sp.fit(data[feat])
            self.sequence_processors[feat] = sp
            self.feature_map.features[feat]['dim'] = len(sp.vocab)
            # build combined vocab mapping (special + normal tokens)
            vocab_map = {tok: idx for tok, idx in sp.vocab.special_tokens.items()}
            vocab_map.update(sp.vocab.vocab)
            self.vocabs[feat] = vocab_map
            pbar.set_postfix_str(f"seq:{feat}")
            pbar.update(1)

        pbar.close()
        self.is_fitted = True
        print(f"âœ… All feature processors fitted! (total: {len(fit_tasks)} features)")
    
    def transform(self, data: pd.DataFrame) -> Dict[str, torch.Tensor]:
        """è½¬æ¢æ•°æ®ä¸ºç‰¹å¾å¼ é‡"""
        if not self.is_fitted:
            raise ValueError("Feature processor must be fitted before transform")
        feats: Dict[str, torch.Tensor] = {}

        # Categorical
        for f in self.feature_map.categorical_features:
            if self.feature_map.is_sequence_feature(f):
                continue
            if f not in data.columns:
                continue
            series = data[f].copy()
            if getattr(series.dtype, 'name', '').startswith('category'):
                if '<UNK>' not in series.cat.categories:
                    series = series.cat.add_categories(['<UNK>'])
                series = series.fillna('<UNK>')
            else:
                series = series.fillna('<UNK>')
            enc = self.categorical_encoders[f].transform(pd.DataFrame({f: series}))
            flat = enc.flatten()
            if flat.size == 0:
                flat = np.array([-1])
            feats[f] = torch.LongTensor(flat)

        # Numerical
        for f in self.feature_map.numerical_features:
            if f not in data.columns:
                continue
            scaled = self.numerical_scalers[f].transform(data[[f]].fillna(0))
            flat = scaled.flatten()
            if flat.size == 0:
                flat = np.array([0.0], dtype=np.float32)
            feats[f] = torch.FloatTensor(flat)

        # Sequence
        for f in self.feature_map.sequence_features:
            if f in data.columns:
                feats[f] = self.sequence_processors[f].transform(data[f])

        return feats
    
    def fit_transform(self, data: pd.DataFrame) -> Dict[str, torch.Tensor]:
        """æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®"""
        self.fit(data)
        return self.transform(data)
    
    def save(self, filepath: str):
        """ä¿å­˜ç‰¹å¾å¤„ç†å™¨"""
        processor_data = {
            'categorical_encoders': self.categorical_encoders,
            'numerical_scalers': self.numerical_scalers,
            'feature_map_features': self.feature_map.features,
            'sequence_processor_configs': {},
            'is_fitted': self.is_fitted
        }
        
        # ä¿å­˜åºåˆ—å¤„ç†å™¨é…ç½®
        for feature_name, processor in self.sequence_processors.items():
            config_path = f"{filepath}.seq_{feature_name}"
            processor.save(config_path)
            processor_data['sequence_processor_configs'][feature_name] = config_path
        
        with open(filepath, 'wb') as f:
            pickle.dump(processor_data, f)
    
    def load(self, filepath: str):
        """åŠ è½½ç‰¹å¾å¤„ç†å™¨"""
        with open(filepath, 'rb') as f:
            processor_data = pickle.load(f)
        
        self.categorical_encoders = processor_data['categorical_encoders']
        self.numerical_scalers = processor_data['numerical_scalers']
        self.feature_map.features = processor_data['feature_map_features']
        self.is_fitted = processor_data['is_fitted']
        
        # åŠ è½½åºåˆ—å¤„ç†å™¨
        self.sequence_processors = {}
        for feature_name, config_path in processor_data['sequence_processor_configs'].items():
            processor = SequenceFeatureProcessor(feature_name=feature_name)
            processor.load(config_path)
            self.sequence_processors[feature_name] = processor


if __name__ == "__main__":
    # 1. å‡†å¤‡æ ·æœ¬æ•°æ®ï¼ˆç”¨äºæ„å»ºè¯æ±‡è¡¨ï¼‰
    # å‡è®¾æˆ‘ä»¬æœ‰ä¸€äº›åˆ†è¯åçš„æ–‡æœ¬åºåˆ—ï¼ˆæ¨¡æ‹ŸçœŸå®è¯­æ–™ï¼‰
    import os
    import json
    from pyspark.sql import SparkSession

    spark = SparkSession.builder \
        .appName("OptimizedGBTExample") \
        .config("spark.driver.memory", "32g") \
        .config("spark.executor.memory", "32g") \
        .config("spark.sql.shuffle.partitions", "8") \
        .getOrCreate()
    pos_sample_path = rf"E:\iserver\model_iserver\dataset\YXH_reasoning_set_M11\train_all.parquet"
    pos_sample_files = [os.path.join(pos_sample_path, f) for f in os.listdir(pos_sample_path) if f.endswith('.parquet')]
    df = spark.read.parquet(*pos_sample_files).limit(60000)

    # è¯»å–JSONæ–‡ä»¶å¹¶æ¢å¤feature_map
    with open("feature.json", "r", encoding="utf-8") as f:
        loaded_feature_map = json.load(f)
    # # 1. åˆå§‹åŒ–ç‰¹å¾æ˜ å°„
    # feature_map = FeatureMap(1,2)
    #
    # # 2. å®šä¹‰ç‰¹å¾ç±»å‹
    feature_map = FeatureMap(5, 6)
    for i in loaded_feature_map['categorical']:
        feature_map.add_feature(i, 'categorical')
    for i in loaded_feature_map['numerical']:
        feature_map.add_feature(i, 'numerical')
    for i in loaded_feature_map['sequence']:
        feature_map.add_feature(i, 'sequence', is_sequence=True)

    # åˆ›å»ºFeatureProcessorå®ä¾‹
    fp = FeatureProcessor(feature_map)

    # æ‹Ÿåˆæ•°æ®ï¼ˆæ„å»ºç¼–ç å™¨ã€æ ‡å‡†åŒ–å™¨ã€è¯æ±‡è¡¨ï¼‰
    data = df.toPandas()
    fp.fit(data)
    # #
    # # # è½¬æ¢æ•°æ®ï¼ˆè¾“å‡ºPyTorchå¼ é‡ï¼‰
    fp.save("./transfrom_pkl/feature_transform.pkl")
    # transformed_data = fp.transform(data)

    fp.load("./transfrom_pkl/feature_transform.pkl")
    transformed_data = fp.transform(data)
    print("è½¬æ¢åçš„ç‰¹å¾ï¼ˆPyTorchå¼ é‡ï¼‰ï¼š")
    for feat_name, tensor in transformed_data.items():
        print(f"\nç‰¹å¾å: {feat_name}")
        print(f"å½¢çŠ¶: {tensor.shape}")
        print(f"å‰3ä¸ªæ ·æœ¬æ•°æ®:\n{tensor[:3]}")
